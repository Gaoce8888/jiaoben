import os
import time
import json
import logging
import requests
import redis
import pymysql
import psycopg2
import pymongo
import pandas as pd
import numpy as np
from sklearn.ensemble import IsolationForest
from sklearn.cluster import DBSCAN
from flask import Flask, request

# é…ç½®æ—¥å¿—
logging.basicConfig(filename='ub_advanced_check.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# è¿æ¥ Redis ç¼“å­˜
redis_client = redis.Redis(host='localhost', port=6379, decode_responses=True)

# MySQL è¿æ¥
db_mysql = pymysql.connect(host="localhost", user="root", password="password", database="ub_db")
cursor_mysql = db_mysql.cursor()

# PostgreSQL è¿æ¥
db_postgres = psycopg2.connect(host="localhost", user="postgres", password="password", database="ub_db")
cursor_postgres = db_postgres.cursor()

# MongoDB è¿æ¥
db_mongo = pymongo.MongoClient("mongodb://localhost:27017/")
mongo_collection = db_mongo["ub_db"]["ub_logs"]

# Flask API ç›‘æ§
app = Flask(__name__)

@app.route("/health", methods=["GET"])
def health_check():
    return json.dumps({"status": "OK"}), 200

# **æ™ºèƒ½æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥**
def check_database_consistency():
    logging.info(">>> å¼€å§‹æ•°æ®åº“ä¸€è‡´æ€§æ£€æŸ¥...")

    cursor_mysql.execute("SELECT COUNT(*) FROM user_behavior")
    mysql_count = cursor_mysql.fetchone()[0]

    cursor_postgres.execute("SELECT COUNT(*) FROM user_behavior")
    postgres_count = cursor_postgres.fetchone()[0]

    mongo_count = mongo_collection.count_documents({})

    logging.info(f"MySQL: {mysql_count}, PostgreSQL: {postgres_count}, MongoDB: {mongo_count}")

    if abs(mysql_count - postgres_count) > 10 or abs(mysql_count - mongo_count) > 10:
        logging.warning("âŒ æ•°æ®åº“æ•°æ®ä¸ä¸€è‡´ï¼å¯èƒ½æœ‰æ•°æ®ä¸¢å¤±")
        return False
    else:
        logging.info("âœ… æ•°æ®ä¸€è‡´")
        return True

# **AI å¼‚å¸¸è¡Œä¸ºæ£€æµ‹**
def detect_abnormal_behavior():
    logging.info(">>> AI å¼‚å¸¸è¡Œä¸ºæ£€æµ‹...")
    cursor_mysql.execute("SELECT user_id, action, timestamp FROM user_behavior")
    data = cursor_mysql.fetchall()
    df = pd.DataFrame(data, columns=['user_id', 'action', 'timestamp'])

    df['timestamp'] = pd.to_datetime(df['timestamp'])
    df['hour'] = df['timestamp'].dt.hour
    action_counts = df.groupby(['user_id', 'hour']).size().reset_index(name='count')

    # **Isolation Forest è¯†åˆ«å¼‚å¸¸è¡Œä¸º**
    model = IsolationForest(contamination=0.02, random_state=42)
    action_counts['anomaly'] = model.fit_predict(action_counts[['count']])

    # **DBSCAN å‘ç°å¼‚å¸¸ç¾¤ä½“**
    dbscan = DBSCAN(eps=5, min_samples=2)
    action_counts['cluster'] = dbscan.fit_predict(action_counts[['count']])

    anomalies = action_counts[action_counts['anomaly'] == -1]
    
    if not anomalies.empty:
        logging.warning(f"âŒ å‘ç°å¼‚å¸¸è¡Œä¸º: \n{anomalies}")
        return False
    else:
        logging.info("âœ… ç”¨æˆ·è¡Œä¸ºæ­£å¸¸")
        return True

# **æ—¥å¿—æ™ºèƒ½åˆ†æ**
def analyze_logs():
    logging.info(">>> åˆ†æ UB ç³»ç»Ÿæ—¥å¿—...")
    error_count = 0
    with open("ub_system.log", "r") as log_file:
        for line in log_file:
            if "ERROR" in line or "Exception" in line:
                error_count += 1
    
    logging.info(f"æ£€æµ‹åˆ° {error_count} æ¡é”™è¯¯æ—¥å¿—")
    if error_count > 5:
        logging.warning("âŒ æ—¥å¿—é”™è¯¯ç‡è¿‡é«˜ï¼Œè¯·æ£€æŸ¥ UB ç³»ç»Ÿ")
        return False
    return True

# **UB API å¥åº·æ£€æŸ¥**
def test_ub_api():
    logging.info(">>> UB API å¥åº·æ£€æŸ¥...")
    try:
        response = requests.get("http://localhost:5000/health")
        if response.status_code == 200:
            logging.info("âœ… UB API æ­£å¸¸")
            return True
        else:
            logging.warning(f"âŒ UB API è¿”å›å¼‚å¸¸çŠ¶æ€ç : {response.status_code}")
            return False
    except Exception as e:
        logging.error(f"âŒ UB API è¿æ¥å¤±è´¥: {e}")
        return False

# **æœåŠ¡å™¨èµ„æºè´Ÿè½½ç›‘æ§**
def check_system_performance():
    logging.info(">>> ç›‘æ§ CPU/å†…å­˜/ç£ç›˜...")
    cpu_usage = os.popen("top -bn1 | grep 'Cpu(s)' | awk '{print $2 + $4}'").read().strip()
    mem_usage = os.popen("free -m | awk 'NR==2{printf \"%s/%sMB (%.2f%%)\", $3,$2,$3*100/$2 }'").read().strip()
    disk_usage = os.popen("df -h / | awk 'NR==2 {print $5}'").read().strip()

    logging.info(f"CPU: {cpu_usage}%, å†…å­˜: {mem_usage}, ç£ç›˜: {disk_usage}")

    if float(cpu_usage) > 80:
        logging.warning("âš ï¸ CPU è¿‡é«˜ï¼")
    if "100%" in mem_usage:
        logging.warning("âš ï¸ å†…å­˜æ»¡è½½ï¼")
    if int(disk_usage.replace("%", "")) > 90:
        logging.warning("âš ï¸ ç£ç›˜å³å°†æ»¡ï¼")

# **è‡ªåŠ¨ä¿®å¤æœºåˆ¶**
def auto_fix():
    logging.info(">>> è§¦å‘è‡ªåŠ¨ä¿®å¤...")
    redis_client.flushall()
    logging.info("âœ… æ¸…ç† Redis ç¼“å­˜")
    
    if not test_ub_api():
        logging.info("ğŸ”„ é‡å¯ UB API æœåŠ¡...")
        os.system("systemctl restart ub_service")
        time.sleep(5)
        if test_ub_api():
            logging.info("âœ… UB API å·²æ¢å¤")
        else:
            logging.error("âŒ UB API ä»ä¸å¯ç”¨")

# **è¿è¡Œæ·±åº¦ä¼˜åŒ– UB æ£€æµ‹**
def run_ub_advanced_check():
    logging.info("====== UB æ·±åº¦ä¼˜åŒ–æ£€æµ‹å¼€å§‹ ======")
    db_check = check_database_consistency()
    behavior_check = detect_abnormal_behavior()
    log_check = analyze_logs()
    api_check = test_ub_api()
    check_system_performance()

    # è§¦å‘ä¿®å¤æœºåˆ¶
    if not db_check or not behavior_check or not log_check or not api_check:
        auto_fix()

    logging.info("====== UB ç³»ç»Ÿæ£€æµ‹å®Œæˆ ======")

# **å¯åŠ¨æ£€æµ‹**
if __name__ == "__main__":
    run_ub_advanced_check()
